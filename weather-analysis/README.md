# üå§Ô∏è Weather Data Processing and Analysis Project

[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/PySpark-3.3.0-orange.svg)](https://spark.apache.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

A comprehensive **Big Data Analytics project** that processes historical weather data using **PySpark** and visualizes climate patterns, temperature trends, and seasonal analysis through an interactive **Streamlit dashboard**.

---

## üìã Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Usage](#-usage)
- [Dashboard Features](#-dashboard-features)
- [Data Pipeline](#-data-pipeline)
- [Analysis Outputs](#-analysis-outputs)
- [Screenshots](#-screenshots)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

---

## üéØ Overview

This project demonstrates **end-to-end Big Data processing** using real-world weather data. It showcases:

- **Distributed data processing** with PySpark
- **ETL pipeline** for data cleaning and transformation
- **Statistical analysis** and correlation studies
- **Interactive visualizations** with Plotly and Streamlit
- **Production-ready code** with error handling and logging

Perfect for data engineering portfolios, academic projects, and learning Big Data technologies!

---

## ‚ú® Features

### üîß Data Processing
- ‚úÖ **Large-scale data handling** - Processes 96,000+ records efficiently
- ‚úÖ **Automated ETL pipeline** - Extract, Transform, Load with PySpark
- ‚úÖ **Data cleaning** - Handles missing values, duplicates, and outliers
- ‚úÖ **Feature engineering** - Creates derived columns (seasons, categories)
- ‚úÖ **Data validation** - Ensures data quality and integrity

### üìä Analysis Capabilities
- ‚úÖ **Temperature trends** - Yearly, monthly, and seasonal patterns
- ‚úÖ **Rainfall analysis** - Precipitation patterns and rainy day tracking
- ‚úÖ **Extreme weather** - Identification of hottest/coldest days
- ‚úÖ **Correlation studies** - Multi-variable relationship analysis
- ‚úÖ **Statistical summaries** - Comprehensive descriptive statistics

### üé® Interactive Dashboard
- ‚úÖ **Real-time filtering** - By year and season
- ‚úÖ **Multiple visualizations** - Line charts, heatmaps, box plots, bar charts
- ‚úÖ **5 analysis tabs** - Temperature, Rainfall, Seasonal, Statistical, Raw Data
- ‚úÖ **Data export** - Download filtered data as CSV
- ‚úÖ **Responsive design** - Works on desktop and mobile

---

## üõ†Ô∏è Tech Stack

| Technology | Purpose | Version |
|------------|---------|---------|
| **Python** | Core programming language | 3.12+ |
| **PySpark** | Distributed data processing | 3.3.0 |
| **Pandas** | Data manipulation | Latest |
| **Streamlit** | Web dashboard framework | 1.28+ |
| **Plotly** | Interactive visualizations | Latest |
| **Java** | Spark runtime environment | 11+ |

---

## üìÅ Project Structure

```
weather-analysis/
‚îÇ
‚îú‚îÄ‚îÄ data/                           # Data directory
‚îÇ   ‚îú‚îÄ‚îÄ weather.csv                 # Raw input data (96,453 rows)
‚îÇ   ‚îú‚îÄ‚îÄ weather_processed.csv       # Cleaned and processed data
‚îÇ   ‚îî‚îÄ‚îÄ analysis_results/           # Analysis outputs
‚îÇ       ‚îú‚îÄ‚îÄ yearly_trends.csv
‚îÇ       ‚îú‚îÄ‚îÄ monthly_trends.csv
‚îÇ       ‚îú‚îÄ‚îÄ seasonal_patterns.csv
‚îÇ       ‚îú‚îÄ‚îÄ rainfall_patterns.csv
‚îÇ       ‚îú‚îÄ‚îÄ temperature_distribution.csv
‚îÇ       ‚îî‚îÄ‚îÄ correlation_matrix.csv
‚îÇ
‚îú‚îÄ‚îÄ src/                            # Source code
‚îÇ   ‚îú‚îÄ‚îÄ data_processor_updated.py   # PySpark ETL pipeline
‚îÇ   ‚îú‚îÄ‚îÄ analysis_updated.py         # Statistical analysis engine
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                    # Helper utilities
‚îÇ
‚îú‚îÄ‚îÄ streamlit_app_updated.py       # Interactive dashboard
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ .gitignore                      # Git ignore rules
‚îú‚îÄ‚îÄ README.md                       # This file
‚îî‚îÄ‚îÄ venv/                          # Virtual environment
```

---

## üöÄ Installation

### Prerequisites

1. **Python 3.12+**
   ```bash
   python --version
   ```

2. **Java 11+** (Required for PySpark)
   ```bash
   java -version
   ```
   
   If not installed:
   - Download from [Adoptium](https://adoptium.net/temurin/releases/)
   - Install Java 11 (LTS)
   - Set `JAVA_HOME` environment variable

### Step-by-Step Setup

1. **Clone the Repository**
   ```bash
   git clone https://github.com/yourusername/weather-analysis.git
   cd weather-analysis
   ```

2. **Create Virtual Environment**
   ```bash
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # Linux/Mac
   source venv/bin/activate
   ```

3. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set Environment Variables**
   
   **Windows (PowerShell):**
   ```powershell
   $env:JAVA_HOME = "C:\Program Files\Eclipse Adoptium\jdk-11.0.28.6-hotspot"
   ```
   
   **Linux/Mac:**
   ```bash
   export JAVA_HOME="/usr/lib/jvm/java-11-openjdk"
   ```

---

## üìñ Usage

### Quick Start (3 Steps)

```bash
# 1. Process the raw data
python src\data_processor_updated.py

# 2. Run statistical analysis
python src\analysis_updated.py

# 3. Launch the dashboard
streamlit run streamlit_app_updated.py
```

The dashboard will open automatically at `http://localhost:8501`

---

### Detailed Workflow

#### **Step 1: Data Processing**

```bash
python src\data_processor_updated.py
```

**What it does:**
- Loads 96,453 weather records
- Cleans and validates data
- Handles missing values and duplicates
- Creates derived features (Season, Temp_Category, Rain_Category)
- Extracts date components (Year, Month, Day)
- Saves processed data to `data/weather_processed.csv`

**Expected output:**
```
============================================================
üå§Ô∏è  WEATHER DATA PROCESSOR
============================================================
‚úÖ Spark Session Created Successfully

üìÇ Loading: ...\data\weather.csv
‚úÖ Loaded: 96453 rows, 12 columns

üßπ Cleaning data...
‚úÖ Cleaned: 4019 rows (removed 92434)

‚ûï Adding features...
‚úÖ Features added

üíæ Saving: ...\data\weather_processed.csv
‚úÖ Saved successfully

============================================================
‚úÖ PROCESSING COMPLETE!
============================================================
```

---

#### **Step 2: Statistical Analysis**

```bash
python src\analysis_updated.py
```

**What it does:**
- Analyzes yearly temperature trends
- Calculates monthly patterns
- Identifies seasonal weather characteristics
- Tracks rainfall patterns
- Finds extreme weather events
- Computes correlation matrices
- Generates 6 analysis CSV files

**Expected output:**
```
============================================================
üå§Ô∏è  WEATHER DATA ANALYSIS
============================================================

üìà Yearly trends...
[Shows temperature statistics by year]

üìÖ Monthly trends...
[Shows temperature statistics by month]

üçÇ Seasonal patterns...
[Shows seasonal averages]

üåßÔ∏è Rainfall patterns...
[Shows precipitation data]

‚ö†Ô∏è Extreme events...
üî• Top 10 Hottest Days
‚ùÑÔ∏è Top 10 Coldest Days

üìä Temperature distribution...
üîó Correlations...

üíæ Saving to: ...\data\analysis_results
‚úÖ yearly_trends.csv
‚úÖ monthly_trends.csv
‚úÖ seasonal_patterns.csv
‚úÖ rainfall_patterns.csv
‚úÖ temperature_distribution.csv
‚úÖ correlation_matrix.csv

============================================================
‚úÖ ANALYSIS COMPLETE!
============================================================
```

---

#### **Step 3: Launch Dashboard**

```bash
streamlit run streamlit_app_updated.py
```

**What it does:**
- Starts local web server
- Opens browser automatically
- Loads processed data and analysis results
- Provides interactive filtering and visualization

**Access at:** `http://localhost:8501`

---

## üé® Dashboard Features

### 1Ô∏è‚É£ **Key Statistics**
Real-time metrics displaying:
- Average Temperature
- Maximum Temperature
- Minimum Temperature
- Total Records (updates based on filters)

### 2Ô∏è‚É£ **Temperature Trends Tab** üå°Ô∏è
- **Daily Temperature Variation** - Interactive line chart
- **Yearly Trends** - Multi-line chart (Avg, Max, Min)
- **Monthly Heatmap** - Temperature across months and years
- **Distribution Histogram** - Temperature frequency distribution

### 3Ô∏è‚É£ **Rainfall Analysis Tab** üåßÔ∏è
- **Monthly Rainfall** - Grouped bar chart by year
- **Rainfall Statistics** - Total, average, rainy days
- **Intensity Distribution** - Pie chart of rain categories

### 4Ô∏è‚É£ **Seasonal Patterns Tab** üçÇ
- **Temperature by Season** - Grouped bar comparison
- **Box Plots** - Temperature distribution per season
- **Seasonal Statistics Table** - Comprehensive seasonal data

### 5Ô∏è‚É£ **Statistical Analysis Tab** üìä
- **Correlation Matrix** - Interactive heatmap
- **Temperature Categories** - Distribution bar chart
- **Summary Statistics** - Descriptive stats table

### 6Ô∏è‚É£ **Raw Data Tab** üìÖ
- **Sortable Data Table** - View and sort any column
- **Row Count Selector** - Display 10, 25, 50, 100, or 500 rows
- **CSV Export** - Download filtered data

---

## üîÑ Data Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Raw CSV Data   ‚îÇ  weather.csv (96,453 rows)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Data Loading   ‚îÇ  PySpark reads CSV with schema inference
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Data Cleaning   ‚îÇ  ‚Ä¢ Remove nulls
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Ä¢ Fill missing values
         ‚îÇ          ‚Ä¢ Cast data types
         ‚îÇ          ‚Ä¢ Remove duplicates
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇFeature Engineer ‚îÇ  ‚Ä¢ Extract date components
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Ä¢ Create Season column
         ‚îÇ          ‚Ä¢ Create Temp_Category
         ‚îÇ          ‚Ä¢ Create Rain_Category
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Save Processed  ‚îÇ  weather_processed.csv (4,019 rows)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Analysis      ‚îÇ  ‚Ä¢ Yearly trends
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Ä¢ Monthly patterns
         ‚îÇ          ‚Ä¢ Seasonal analysis
         ‚îÇ          ‚Ä¢ Correlations
         ‚îÇ          ‚Ä¢ Extremes
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Analysis CSVs   ‚îÇ  6 result files
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Dashboard     ‚îÇ  Interactive Streamlit app
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìà Analysis Outputs

### Generated Files

| File | Description | Rows | Columns |
|------|-------------|------|---------|
| `yearly_trends.csv` | Temperature stats by year | 12 | 5 |
| `monthly_trends.csv` | Temperature stats by month | 144 | 5 |
| `seasonal_patterns.csv` | Seasonal averages | 4 | 6 |
| `rainfall_patterns.csv` | Monthly precipitation | 144 | 5 |
| `temperature_distribution.csv` | Temp category counts | 5 | 3 |
| `correlation_matrix.csv` | Variable correlations | 5 | 6 |

### Sample Insights

**Yearly Temperature Trends:**
```
Year | Avg_Temperature | Max_Temperature | Min_Temperature | Record_Count
-----|-----------------|-----------------|-----------------|-------------
2006 |          10.33  |          27.01  |         -11.22  |         365
2007 |          11.27  |          30.88  |          -7.52  |         365
2008 |          11.31  |          30.82  |         -11.02  |         366
```

**Seasonal Patterns:**
```
Season | Avg_Temperature | Avg_Humidity | Avg_Wind_Speed | Total_Precipitation
-------|-----------------|--------------|----------------|--------------------
Spring |          11.25  |         0.72 |          10.50 |             2530.0
Summer |          20.60  |         0.71 |           7.81 |             2530.0
Autumn |          10.93  |         0.81 |           8.69 |             2502.5
Winter |           1.26  |         0.86 |          11.05 |             2485.0
```

---

## üì∏ Screenshots

### Dashboard Overview
![Dashboard](screenshots/dashboard_overview.png)

### Temperature Trends
![Temperature](screenshots/temperature_trends.png)

### Statistical Analysis
![Statistics](screenshots/statistical_analysis.png)

---

## üêõ Troubleshooting

### Common Issues

#### 1. **Java Not Found Error**
```
Error: The system cannot find the path specified
```

**Solution:**
```bash
# Install Java 11
# Download from: https://adoptium.net/temurin/releases/

# Set JAVA_HOME
# Windows:
$env:JAVA_HOME = "C:\Program Files\Eclipse Adoptium\jdk-11.0.28.6-hotspot"

# Linux/Mac:
export JAVA_HOME="/usr/lib/jvm/java-11-openjdk"
```

#### 2. **PySpark Version Mismatch**
```
Error: class file version 61.0
```

**Solution:**
```bash
# Downgrade PySpark to work with Java 11
pip install pyspark==3.3.0
```

#### 3. **Module Not Found: 'distutils'**
```
ModuleNotFoundError: No module named 'distutils'
```

**Solution:**
```bash
pip install setuptools
```

#### 4. **Streamlit ScriptRunContext Warning**
```
WARNING: Thread 'MainThread': missing ScriptRunContext
```

**Solution:**
```bash
# Run with streamlit command (NOT python)
streamlit run streamlit_app_updated.py
```

#### 5. **Port Already in Use**
```
Error: Port 8501 is already in use
```

**Solution:**
```bash
# Use different port
streamlit run streamlit_app_updated.py --server.port 8502
```

---

## üîß Configuration

### requirements.txt
```txt
pyspark==3.3.0
pandas>=2.0.0
streamlit>=1.28.0
plotly>=5.17.0
setuptools>=68.0.0
```

### Environment Variables
```bash
# Required
JAVA_HOME=/path/to/java

# Optional (for optimization)
SPARK_HOME=/path/to/spark
HADOOP_HOME=/path/to/hadoop
```

---

## üìä Dataset Information

### Source
Historical weather data from **NOAA** (National Oceanic and Atmospheric Administration)

### Specifications
- **Records**: 96,453 raw entries
- **Processed**: 4,019 unique daily records
- **Time Period**: 2006-2017
- **Location**: Global weather stations
- **Format**: CSV with headers

### Columns

| Column | Type | Description |
|--------|------|-------------|
| Date | Date | Observation date |
| Temperature | Float | Temperature in Celsius |
| Humidity | Float | Humidity percentage (0-1) |
| Wind_Speed | Float | Wind speed in km/h |
| Pressure | Float | Atmospheric pressure in millibars |
| Precipitation | Float | Daily precipitation in mm |
| Season | String | Calculated season (Spring/Summer/Autumn/Winter) |
| Temp_Category | String | Temperature classification |

---

## üéì Learning Outcomes

This project demonstrates:

1. **Big Data Processing** - Handling large datasets with PySpark
2. **ETL Pipelines** - Extract, Transform, Load workflows
3. **Data Cleaning** - Handling missing values and outliers
4. **Feature Engineering** - Creating derived columns
5. **Statistical Analysis** - Descriptive and correlation analysis
6. **Data Visualization** - Interactive charts with Plotly
7. **Web Development** - Building dashboards with Streamlit
8. **Production Code** - Error handling, logging, and documentation

---

## üöÄ Future Enhancements

- [ ] Add machine learning models for weather prediction
- [ ] Implement time series forecasting (ARIMA, Prophet)
- [ ] Add map-based visualizations for geographical data
- [ ] Integrate real-time weather API data
- [ ] Deploy to cloud (AWS, Azure, GCP)
- [ ] Add authentication and user management
- [ ] Implement caching for better performance
- [ ] Add more statistical tests and anomaly detection

---

## ü§ù Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Development Setup
```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Format code
black src/
flake8 src/
```

---

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üë®‚Äçüíª Author

**Your Name**
- GitHub: [@yourusername](https://github.com/yourusername)
- LinkedIn: [Your Profile](https://linkedin.com/in/yourprofile)
- Email: your.email@example.com

---

## üôè Acknowledgments

- **Apache Spark** for distributed computing framework
- **Streamlit** for amazing dashboard capabilities
- **Plotly** for interactive visualizations
- **NOAA** for providing weather data
- **Python Community** for excellent libraries and support

---

## üìö References

- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [Plotly Python](https://plotly.com/python/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)

---

## ‚≠ê Star This Repository

If you found this project helpful, please consider giving it a star! ‚≠ê

---

**Built with ‚ù§Ô∏è using Python, PySpark, and Streamlit**

Last Updated: October 31, 2025
